{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Playground for a Basic Neural Network\n",
    "\n",
    "This notebook provides the underlying code for the concepts we saw in the Manim visualization. While the animation gives us the visual intuition, this playground lets you get your hands dirty and see how the numbers actually flow.\n",
    "\n",
    "We'll build the entire neural network from scratch using only Python and NumPy, focusing on the core mechanics of **feedforward propagation**.\n",
    "\n",
    "**Goal:** To understand how a given set of inputs, weights, and biases produces a final output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: The Activation Function (The \"Squisher\")\n",
    "\n",
    "First, let's define the sigmoid activation function. As we saw in the animation, its job is to take any number and \"squish\" it into a value between 0 and 1. This is useful for representing things like probabilities or the \"activation level\" of a neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sigmoid of 10: 1.0000\n",
      "Sigmoid of -8: 0.0003\n",
      "Sigmoid of 0: 0.5000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"This is our activation function. It squishes any value into the (0, 1) range.\"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Let's test it out!\n",
    "large_positive_number = 10\n",
    "large_negative_number = -8\n",
    "zero = 0\n",
    "\n",
    "print(f\"Sigmoid of {large_positive_number}: {sigmoid(large_positive_number):.4f}\") # Should be close to 1\n",
    "print(f\"Sigmoid of {large_negative_number}: {sigmoid(large_negative_number):.4f}\") # Should be close to 0\n",
    "print(f\"Sigmoid of {zero}: {sigmoid(zero):.4f}\")                 # Should be exactly 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: The Neuron\n",
    "\n",
    "Now, let's model a single neuron. A neuron takes a set of inputs, multiplies each by a corresponding weight, sums them up, adds a bias, and finally applies the activation function.\n",
    "\n",
    "$$ \\text{output} = \\sigma \\left( \\sum (\\text{inputs} \\cdot \\text{weights}) + \\text{bias} \\right) $$\n",
    "\n",
    "We'll create a Python class to represent this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The neuron's activation for a vertical line is: 0.9933\n",
      "The neuron's activation for a horizontal line is: 0.5000\n"
     ]
    }
   ],
   "source": [
    "class Neuron:\n",
    "    \"\"\"A single neuron with a set of weights and a bias.\"\"\"\n",
    "    def __init__(self, weights, bias):\n",
    "        self.weights = weights\n",
    "        self.bias = bias\n",
    "    \n",
    "    def feedforward(self, inputs):\n",
    "        \"\"\"This is where the calculation happens!\"\"\"\n",
    "        # 1. Calculate the weighted sum\n",
    "        total = np.dot(self.weights, inputs) + self.bias\n",
    "        \n",
    "        # 2. Apply the activation function\n",
    "        return sigmoid(total)\n",
    "\n",
    "# --- Let's Play! ---\n",
    "\n",
    "# Recreate the neuron we focused on in the animation.\n",
    "# It had 9 inputs, so it needs 9 weights.\n",
    "weights = np.array([-0.5, 2.0, -0.5, -0.5, 2.0, -0.5, -0.5, 2.0, -0.5]) # Weights for detecting a vertical line\n",
    "bias = -1.0  # <-- Try changing this! A more negative bias makes the neuron harder to activate.\n",
    "\n",
    "neuron = Neuron(weights, bias)\n",
    "\n",
    "# This was our input image of a vertical line\n",
    "vertical_line_input = np.array([0, 1, 0, 0, 1, 0, 0, 1, 0]) # <-- Try changing the input!\n",
    "\n",
    "# Let's see what the neuron's activation is for this input\n",
    "output = neuron.feedforward(vertical_line_input)\n",
    "\n",
    "print(f\"The neuron's activation for a vertical line is: {output:.4f}\")\n",
    "\n",
    "# What about for a horizontal line? The activation should be much lower.\n",
    "horizontal_line_input = np.array([0, 0, 0, 1, 1, 1, 0, 0, 0])\n",
    "output_horizontal = neuron.feedforward(horizontal_line_input)\n",
    "print(f\"The neuron's activation for a horizontal line is: {output_horizontal:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: The Full Network\n",
    "\n",
    "Now let's assemble our full network. It's just a collection of layers, and each layer is a collection of neurons. We'll create a class for the whole network that connects the layers.\n",
    "\n",
    "Our network structure is:\n",
    "- **Input Layer:** 9 neurons (representing the 3x3 grid)\n",
    "- **Hidden Layer:** 4 neurons\n",
    "- **Output Layer:** 2 neurons (one for 'P(Vertical)', one for 'P(Horizontal)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for Vertical Line: [0.83192724 0.87373214]\n",
      "(Output 1: P(Vertical), Output 2: P(Horizontal))\n",
      "\n",
      "Prediction for Horizontal Line: [0.83351873 0.87197101]\n",
      "\n",
      "Prediction for Diagonal Line: [0.83198452 0.8734924 ]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class OurNeuralNetwork:\n",
    "    \"\"\"\n",
    "    A neural network with:\n",
    "      - 9 inputs\n",
    "      - a hidden layer with 4 neurons\n",
    "      - an output layer with 2 neurons\n",
    "    Each neuron has weights and a bias, which we'll initialize randomly for this example.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # Set a seed for reproducibility, so we get the same random numbers every time\n",
    "        np.random.seed(42)\n",
    "        \n",
    "        # --- Hidden Layer (4 neurons) ---\n",
    "        # Each neuron needs 9 weights (one for each input)\n",
    "        h1_weights = np.random.rand(9)\n",
    "        h1_bias = np.random.rand()\n",
    "        self.h1 = Neuron(h1_weights, h1_bias)\n",
    "        \n",
    "        h2_weights = np.random.rand(9)\n",
    "        h2_bias = np.random.rand()\n",
    "        self.h2 = Neuron(h2_weights, h2_bias)\n",
    "\n",
    "        h3_weights = np.random.rand(9)\n",
    "        h3_bias = np.random.rand()\n",
    "        self.h3 = Neuron(h3_weights, h3_bias)\n",
    "\n",
    "        h4_weights = np.random.rand(9)\n",
    "        h4_bias = np.random.rand()\n",
    "        self.h4 = Neuron(h4_weights, h4_bias)\n",
    "        \n",
    "        # --- Output Layer (2 neurons) ---\n",
    "        # Each neuron needs 4 weights (one for each output from the hidden layer)\n",
    "        o1_weights = np.random.rand(4)\n",
    "        o1_bias = np.random.rand()\n",
    "        self.o1 = Neuron(o1_weights, o1_bias)\n",
    "        \n",
    "        o2_weights = np.random.rand(4)\n",
    "        o2_bias = np.random.rand()\n",
    "        self.o2 = Neuron(o2_weights, o2_bias)\n",
    "        \n",
    "    def feedforward(self, x):\n",
    "        # 1. Get the outputs from the hidden layer\n",
    "        out_h1 = self.h1.feedforward(x)\n",
    "        out_h2 = self.h2.feedforward(x)\n",
    "        out_h3 = self.h3.feedforward(x)\n",
    "        out_h4 = self.h4.feedforward(x)\n",
    "        \n",
    "        # This is the input for the output layer\n",
    "        hidden_layer_output = np.array([out_h1, out_h2, out_h3, out_h4])\n",
    "        \n",
    "        # 2. Get the final outputs from the output layer\n",
    "        out_o1 = self.o1.feedforward(hidden_layer_output)\n",
    "        out_o2 = self.o2.feedforward(hidden_layer_output)\n",
    "        \n",
    "        return np.array([out_o1, out_o2])\n",
    "\n",
    "# Create an instance of our network\n",
    "network = OurNeuralNetwork()\n",
    "\n",
    "# --- Let's Test the Whole Network! ---\n",
    "\n",
    "# Input 1: The vertical line\n",
    "vertical_line = np.array([0, 1, 0, 0, 1, 0, 0, 1, 0])\n",
    "prediction_vertical = network.feedforward(vertical_line)\n",
    "print(f\"Prediction for Vertical Line: {prediction_vertical}\")\n",
    "print(f\"(Output 1: P(Vertical), Output 2: P(Horizontal))\\n\")\n",
    "\n",
    "# Input 2: The horizontal line\n",
    "horizontal_line = np.array([0, 0, 0, 1, 1, 1, 0, 0, 0])\n",
    "prediction_horizontal = network.feedforward(horizontal_line)\n",
    "print(f\"Prediction for Horizontal Line: {prediction_horizontal}\\n\")\n",
    "\n",
    "# Input 3: A diagonal line\n",
    "diagonal_line = np.array([1, 0, 0, 0, 1, 0, 0, 0, 1])\n",
    "prediction_diagonal = network.feedforward(diagonal_line)\n",
    "print(f\"Prediction for Diagonal Line: {prediction_diagonal}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Final Thought: Where's the \"Learning\"?\n",
    "\n",
    "You probably noticed that the network's predictions are pretty random. The first output isn't always highest for the vertical line, and the second isn't always highest for the horizontal line. **This is expected!**\n",
    "\n",
    "Why? Because we initialized its weights and biases with *random* numbers. The network hasn't learned anything yet.\n",
    "\n",
    "The process of **training** a neural network is the process of showing it many examples (e.g., thousands of vertical and horizontal lines) and systematically adjusting the weights and biases so that its predictions get better and better.\n",
    "\n",
    "That process, often done with algorithms like **backpropagation** and **gradient descent**, is the story for another time. For now, you have a solid, hands-on understanding of how an already-trained network makes a prediction."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
